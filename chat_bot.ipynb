{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import langchain\n",
    "import faiss\n",
    "import pathlib\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "import re\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "import os\n",
    "from getpass import getpass\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = getpass()  #insert OpenAI API key here\n",
    "\n",
    "#DOCS_FOLDER = \"docs\"  # Folder to check out to\n",
    "#REPO_DOCUMENTS_PATH = \"collections/_products/categories/devops/ship-hats\"  # Set to \"\" to index the whole data folder\n",
    "DOCUMENT_BASE_URL = \"https://www.developer.tech.gov.sg/products/categories/devops/ship-hats\"  # Actual URL\n",
    "DATA_STORE_DIR = \"data_store\"  # Folder to save/load the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DOCS_FOLDER = \"test\"  # Folder to check out to\n",
    "REPO_DOCUMENTS_PATH = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "name_filter = \"**/*.md\"\n",
    "separator = \"\\n### \"  # This separator assumes Markdown docs from the repo uses ### as logical main header most of the time\n",
    "chunk_size_limit = 1000\n",
    "max_chunk_overlap = 20\n",
    "\n",
    "repo_path = pathlib.Path(os.path.join(DOCS_FOLDER, REPO_DOCUMENTS_PATH))\n",
    "document_files = list(repo_path.glob(name_filter))\n",
    "\n",
    "def convert_path_to_doc_url(doc_path):\n",
    "  # Convert from relative path to actual document url\n",
    "  return re.sub(f\"{DOCS_FOLDER}/{REPO_DOCUMENTS_PATH}/(.*)\\.[\\w\\d]+\", f\"{DOCUMENT_BASE_URL}/\\\\1\", str(doc_path))\n",
    "\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=open(file, \"r\").read(),\n",
    "        metadata={\"source\": convert_path_to_doc_url(file)}\n",
    "    )\n",
    "    for file in document_files\n",
    "]\n",
    "\n",
    "text_splitter = CharacterTextSplitter(separator=separator, chunk_size=chunk_size_limit, chunk_overlap=max_chunk_overlap)\n",
    "split_docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('test/README_22.md')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total word count: 255\n",
      "\n",
      "Estimated tokens: 345\n",
      "\n",
      "Estimated cost of embedding: $0.00013800000000000002\n"
     ]
    }
   ],
   "source": [
    "# create a GPT-4 encoder instance\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "\n",
    "total_word_count = sum(len(doc.page_content.split()) for doc in split_docs)\n",
    "total_token_count = sum(len(enc.encode(doc.page_content)) for doc in split_docs)\n",
    "\n",
    "print(f\"\\nTotal word count: {total_word_count}\")\n",
    "print(f\"\\nEstimated tokens: {total_token_count}\")\n",
    "print(f\"\\nEstimated cost of embedding: ${total_token_count * 0.0004 / 1000}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "vector_store = FAISS.from_documents(split_docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "vector_store.save_local(DATA_STORE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "## Search results:\n",
       "\n",
       "\n",
       "  ### Source:\n",
       "test\\README_22.md\n",
       "\n",
       "  #### Score:\n",
       "0.6939088106155396\n",
       "\n",
       "  #### Content:\n",
       "# Text Split Explorer\n",
       "\n",
       "![ui.png](ui.png)\n",
       "\n",
       "Many of the most important LLM applications involve connecting LLMs to external sources of data.\n",
       "A prerequisite to doing this is to ingest data into a format where LLMs can easily connect to them.\n",
       "Most of the time, that means ingesting data into a vectorstore.\n",
       "A prerequisite to doing this is to split the original text into smaller chunks.\n",
       "\n",
       "While this may seem trivial, it is a nuanced and overlooked step.\n",
       "When splitting text, you want to ensure that each chunk has cohesive information - e.g. you don't just want to split in the middle of sentence.\n",
       "What \"cohesive information\" means can differ depending on the text type as well.\n",
       "For example, with Markdown you have section delimiters (`##`) so you may want to keep those together, while for splitting Python code you may want to keep all classes and methods together (if possible).\n",
       "\n",
       "This repo (and associated Streamlit app) are designed to help explore different types of text splitting.\n",
       "You can adjust different parameters and choose different types of splitters.\n",
       "By pasting a text file, you can apply the splitter to that text and see the resulting splits.\n",
       "You are also shown a code snippet that you can copy and use in your application\n",
       "\n",
       "## Hosted App\n",
       "\n",
       "To use the hosted app, head to [https://langchain-text-splitter.streamlit.app/](https://langchain-text-splitter.streamlit.app/)\n",
       "\n",
       "## Running locally\n",
       "\n",
       "To run locally, first set up the environment by cloning the repo and running:\n",
       "\n",
       "```shell\n",
       "pip install -r requirements\n",
       "```\n",
       "\n",
       "Then, run the Streamlit app with:\n",
       "\n",
       "```shell\n",
       "streamlit run splitter.py\n",
       "```\n",
       "\n",
       "  \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "search_result = vector_store.similarity_search_with_score(\"What is SHIP-HATS?\")\n",
    "\n",
    "line_separator = \"\\n\"# {line_separator}Source: {r[0].metadata['source']}{line_separator}Score:{r[1]}{line_separator}\n",
    "display(Markdown(f\"\"\"\n",
    "## Search results:{line_separator}\n",
    "{line_separator.join([\n",
    "  f'''\n",
    "  ### Source:{line_separator}{r[0].metadata['source']}{line_separator}\n",
    "  #### Score:{line_separator}{r[1]}{line_separator}\n",
    "  #### Content:{line_separator}{r[0].page_content}{line_separator}\n",
    "  '''\n",
    "  for r in search_result\n",
    "])}\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "system_template=\"\"\"Use the following pieces of context to answer the users question.\n",
    "Take note of the sources and include them in the answer in the format: \"SOURCES: source1 source2\", use \"SOURCES\" in capital letters regardless of the number of sources.\n",
    "If you don't know the answer, just say that \"I don't know\", don't try to make up an answer.\n",
    "----------------\n",
    "{summaries}\"\"\"\n",
    "messages = [\n",
    "    SystemMessagePromptTemplate.from_template(system_template),\n",
    "    HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "]\n",
    "prompt = ChatPromptTemplate.from_messages(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "\n",
    "chain_type_kwargs = {\"prompt\": prompt}\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, max_tokens=256)  # Modify model_name if you have access to GPT-4\n",
    "chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vector_store.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs=chain_type_kwargs\n",
    ")\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "def print_result(result):\n",
    "  output_text = f\"\"\"### Question:\n",
    "  {query}\n",
    "  ### Answer:\n",
    "  {result['answer']}\n",
    "  ### Sources:\n",
    "  {result['sources']}\n",
    "  ### All relevant sources:\n",
    "  {' '.join(list(set([doc.metadata['source'] for doc in result['source_documents']])))}\n",
    "  \"\"\"\n",
    "  display(Markdown(output_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Question:\n",
       "  What is SHIP-HATS?\n",
       "  ### Answer:\n",
       "  I don't know what SHIP-HATS is.\n",
       "  ### Sources:\n",
       "  \n",
       "  ### All relevant sources:\n",
       "  test\\README_22.md\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"What is SHIP-HATS?\"\n",
    "result = chain(query)\n",
    "print_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Question:\n",
       "  What is the purpose of splitting text when using LLMs?\n",
       "  ### Answer:\n",
       "  The purpose of splitting text when using LLMs (Language Model Models) is to ingest data into a format where LLMs can easily connect to them. Splitting text into smaller chunks allows LLMs to process and analyze the data more efficiently. It ensures that each chunk of text contains cohesive information, such as keeping sentences or specific sections together, depending on the text type. This step is crucial for connecting LLMs to external sources of data and enabling them to perform tasks like language generation, translation, summarization, and more. \n",
       "\n",
       "\n",
       "  ### Sources:\n",
       "  test\\README_22.md\n",
       "  ### All relevant sources:\n",
       "  test\\README_22.md\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"What is the purpose of splitting text when using LLMs?\"\n",
    "result = chain(query)\n",
    "print_result(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
